{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "DF = pd.read_csv(\"saved_tweets.csv\",header=None)  \n",
    "\n",
    "DF.columns = [\"content\",\"hashtags\",\"screen_name\",\"location\"]\n",
    "#print(DF)\n",
    "\n",
    "print(DF.iloc[774,:][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  \n",
    "import ast\n",
    "\n",
    "# Extract hashtags and put them in a list\n",
    "list_hashtag_strings = [entry for entry in DF.hashtags]  \n",
    "list_hashtag_lists = ast.literal_eval(','.join(list_hashtag_strings))  \n",
    "hashtag_list = [ht.lower() for list_ in list_hashtag_lists for ht in list_]\n",
    "\n",
    "# Count most common hashtags\n",
    "counter_hashtags = Counter(hashtag_list)  \n",
    "counter_hashtags.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "# help(WordCloud)\n",
    "\n",
    "def tag_cloud(tokens, stop_set = None):\n",
    "    wc = WordCloud(stopwords = stop_set).generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(12,12),dpi=200)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    #plt.savefig('wordcloud.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "tokens = list()\n",
    "for s in DF[\"content\"]:\n",
    "    tokens.append([token.lower() for token in s.split()])\n",
    "\n",
    "tokens = [token for sublist in tokens for token in sublist]\n",
    "    \n",
    "\n",
    "tag_cloud(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### affective computing: dictionary-based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vect(tokens,center=False):\n",
    "    \"\"\"\n",
    "    Lab MT sentiment score for lists of tokens in list tokens\n",
    "    - scores are zero-centered\n",
    "    \"\"\"\n",
    "    labmt = pd.read_csv(\"labmt_dict.csv\", \n",
    "                        sep=\"\\t\", encoding=\"utf-8\", index_col=0)\n",
    "    if center:\n",
    "        avg = labmt.happiness_average.mean()\n",
    "        sent_dict = (labmt.happiness_average - avg).to_dict()\n",
    "    else:\n",
    "        sent_dict = (labmt.happiness_average).to_dict()\n",
    "    out = [sent_dict.get(token,0.0) for token in tokens]\n",
    "    \n",
    "    return out\n",
    "\n",
    "content = DF[\"content\"].tolist()\n",
    "\n",
    "sentiment_array = []\n",
    "for s in content:\n",
    "    tokens = [token.lower() for token in s.split()]\n",
    "    score = sum(sent_vect(tokens))/len(tokens)\n",
    "    sentiment_array.append(score)\n",
    "\n",
    "DF[\"sentiment\"] = sentiment_array\n",
    "DF.to_csv(\"saved_tweets_annotated.csv\",index=False)\n",
    "\n",
    "print(DF.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def tokenize(s,n=0):\n",
    "    pattern = re.compile(r\"\\W+\")\n",
    "    return [token for token in pattern.split(s) if len(token) > n]\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "content = DF[\"content\"].tolist()        \n",
    "# vector space\n",
    "no_features = 1000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(content)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# model\n",
    "no_topics = 5\n",
    "mdl = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "\n",
    "nmf = mdl.fit(tfidf)\n",
    "\n",
    "W = mdl.fit_transform(tfidf)\n",
    "H = mdl.components_\n",
    "\n",
    "\n",
    "# document representation\n",
    "W = mdl.fit_transform(tfidf)\n",
    "# dictionary\n",
    "H = mdl.components_\n",
    "\n",
    "# inspect\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(W)\n",
    "\n",
    "print(pca.explained_variance_ratio_)  \n",
    "print(pca.singular_values_)  \n",
    "\n",
    "\n",
    "W2d = pca.transform(W)\n",
    "\n",
    "color_list = list()\n",
    "threshold = sum(sentiment_array)/len(sentiment_array)\n",
    "for val in sentiment_array:\n",
    "    if val < threshold:\n",
    "        color_list.append(\"b\")\n",
    "    elif val > threshold:\n",
    "        color_list.append(\"r\")\n",
    "    else:\n",
    "        color_list.append(\"y\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(W2d[:,0], W2d[:,1],c = color_list);\n",
    "plt.axis([-.04,.075,-.075,.075])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from natsort import natsorted\n",
    "\n",
    "fnames = natsorted(glob.glob(os.path.join(\"russian-troll\",\"*.csv\")))\n",
    "\n",
    "df = list()\n",
    "for fname in fnames[:3]:\n",
    "    df.append(pd.read_csv(fname,low_memory=False))\n",
    "df = pd.concat(df,axis=0)\n",
    "\n",
    "content = df[\"content\"].tolist()\n",
    "\n",
    "# vector space\n",
    "no_features = 1000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(content)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# model\n",
    "no_topics = 25\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "# inspect\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
